---
title: "PMS Study"
author: "Mitchel Kappen & Sofie Raeymakers"
date: "`r Sys.setlocale('LC_TIME', 'C'); format(Sys.time(), '%d\\\\. %B %Y')`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PMS Study

This is the data processing html for the PMS study

```{r libraries, include = FALSE}
# we will first write some pseudocode to describe what we want to do
#upload files
#find best linear model to represent data with AI

#     PSS ~ PMSScore*Moment + (1|Subject)
# 
#     PSS deppendent variablle
#     pMS = 1 geen PMS
#     keer moment: moment v afname
#     1 out of subject: random effect toevoegen
#     straks factoren vd PMS score en van moment: hoeveel levels? 
#       formules van doen
#     laten weten welke formule beste
#     

    
#### Settings & Packages #### -----------------------------------------------------
rm(list = ls())
options(contrasts = c("contr.sum","contr.poly")) #use this for the p value of the t test
library(lme4)
library(lmerTest)
library(effects)
library(dplyr)
library(car)
library(emmeans)
library(fitdistrplus)
library(dplyr)
library(car)
# library(MuMIn)
library(ggplot2)
library(ggstatsplot)
library(ggsignif)
library(gridExtra)
library(tidyverse)
library(ggeffects)
library(pander)
# library(viridis)
library (yarrr)
library(knitr)
recode <- dplyr::recode
# Suppress summarize info
options(dplyr.summarise.inform = FALSE)
count <- dplyr::count 
# set the theme to theme_bw for all ggplot2 figures
theme_set(theme_bw())
# create folder to save figures
if (!dir.exists("figures")) dir.create("figures")


# General settings
nAGQ = 0 # When writing code, set to 0, when getting final results, set to 1
```


```{r load-data, echo=FALSE, results='asis'}
#### IMPORT DATA & INSPECTION #### -------------------------------------------------------------
knitr::opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))# Set working directory to current directory
# setwd("C:\Users\ASUSTeK\OneDrive\2021-2022\internship\projects")
data <- read.csv("Data/allPMSdata.csv", header=TRUE)
# data <- read.table("Data/allPMSdata.csv",sep="\t", header=TRUE)


# from wide to long
#check if subject column is a facctor
#str(data)
#head(data)
# data <-as.numeric(data)
data$Subject <- factor(data$ID)
#typeof(data$Subject)


#we make a new variable that has value 1 for the first TestMoment and 2 for the second TestMoment
#These moments were counterbalanced
#when the order was B-A and the moment is B, this means it is the first test moment
#and vice versa for A-B and moment A. 

# TestMoment 1 == Follicular phase
# TestMoment 2 == Luteal phase
data$TestMoment[data$Order == "A-B" & data$Moment == "A"] = 1
data$TestMoment[data$Order == "B-A" & data$Moment == "A"] = 2
data$TestMoment[data$Order == "A-B" & data$Moment == "B"] = 2
data$TestMoment[data$Order == "B-A" & data$Moment == "B"] = 1
#check if there are still values missing (NA)
#sum(is.na(data$TestMoment))

# new variable PMSSCORE NEW iedereen pms 0 ook 0 iedereen die 1 OF 2 heeft wordt 1, 
data$PMSScoreNew[data$PMSScore==0] = 'noPMS'
data$PMSScoreNew[data$PMSScore==1] = 'PMS'
data$PMSScoreNew[data$PMSScore==2] = 'PMS'
#sum(is.na(data$PMSScoreNew))


# Check whether R recognizes the variable types correctly
#we make factors of the independable variables

data$PMSScore <- factor(data$PMSScore)
data$PMSScoreNew <- factor(data$PMSScoreNew)

data$Moment <- factor(data$TestMoment) # This removes "A and B", A == 1, B == 2 now

# Exclude data?
dataBig = data # Saved all the data here
data = data[, -which(names(data) == "X" | names(data) == "Stimulus" | names(data) == "Valence" | names(data) == "Arousal" | names(data) == "rt")]
data = distinct(data)

# Define the formula for the model & check which model fits the data best
Formula <- PSS ~ PMSScoreNew*TestMoment + (1|Subject)


d0.1 <- lmer(Formula,data=data)
d0.2 <- glmer(Formula,data=data, family = gaussian(link = "inverse"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)
d0.3 <- glmer(Formula,data=data, family = gaussian(link = "log"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)

d0.4 <- glmer(Formula,data=data, family = Gamma(link = "identity"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)
d0.5 <- glmer(Formula,data=data, family = Gamma(link = "inverse"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)
d0.6 <- glmer(Formula,data=data, family = Gamma(link = "log"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)

d0.7 <- glmer(Formula,data=data, family = inverse.gaussian(link = "identity"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)
d0.8 <- glmer(Formula,data=data, family = inverse.gaussian(link = "inverse"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)
d0.9 <- glmer(Formula,data=data, family = inverse.gaussian(link = "log"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=nAGQ)


# Remove the models that produced errors in the line below, the one with the lowest value is the best fitting model
tabel <- cbind(AIC(d0.1), AIC(d0.2), AIC(d0.3), AIC(d0.4), AIC(d0.5), AIC(d0.6), AIC(d0.7), AIC(d0.8), AIC(d0.9))
#show table in html
kable (tabel, caption = "table showing the different AIC")
# utils::View(tabel)
# d0.1 has the lowest AIC. so linear model is the best fitting model

```

d0.1 <- lmer(Formula,data=data)
d0.2 <- glmer(Formula,data=data, family = gaussian(link = "inverse"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)
d0.3 <- glmer(Formula,data=data, family = gaussian(link = "log"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)

d0.4 <- glmer(Formula,data=data, family = Gamma(link = "identity"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)
d0.5 <- glmer(Formula,data=data, family = Gamma(link = "inverse"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)
d0.6 <- glmer(Formula,data=data, family = Gamma(link = "log"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)

d0.7 <- glmer(Formula,data=data, family = inverse.gaussian(link = "identity"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)
d0.8 <- glmer(Formula,data=data, family = inverse.gaussian(link = "inverse"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)
d0.9 <- glmer(Formula,data=data, family = inverse.gaussian(link = "log"),glmerControl(optimizer= "bobyqa", optCtrl = list(maxfun = 100000)),nAGQ=0)


After evaluating different linear models, we found that the best fitting model for the data is d0.4. 

Model: PSS ~ PMSScoreNew*TestMoment + (1|Subject)


## Anova test

``` {r Statistics, echo = FALSE}
Anova(d0.4) 
# Run Anova, double square brackets because of list properties
# Analysis of Deviance Table (Type II Wald chisquare tests)
# 
# Response: PSS
#                          Chisq Df Pr(>Chisq)    
# PMSScoreNew             17.199  1  3.367e-05 ***
# TestMoment             245.477  1  < 2.2e-16 ***
# PMSScoreNew:TestMoment 130.346  1  < 2.2e-16 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


```


## pairwise ~Testmoment emmeans
```{r echo=FALSE}
emmeans(d0.1, pairwise ~ TestMoment*PMSScoreNew , adjust ="fdr", type="response")
#   
#  TestMoment emmean    SE  df asymp.LCL asymp.UCL
#           1   30.2 0.142 Inf      30.0      30.5
#           2   29.8 0.143 Inf      29.5      30.1
# 
# Results are averaged over the levels of: PMSScoreNew 
# Degrees-of-freedom method: asymptotic 
# Confidence level used: 0.95 
# 
# $contrasts
#  contrast estimate     SE  df z.ratio p.value
#  1 - 2       0.429 0.0326 Inf  13.146  <.0001
# 
# Results are averaged over the levels of: PMSScoreNew 
# Degrees-of-freedom method: asymptotic 

```


## Plots

Here are some plots:

```{r Plots, echo=FALSE}



ggplot(data = data) + 
  geom_bar(mapping = aes(x = PSS))

##PIRATE PLOT:

# Plotting
  dpi=600    #pixels per square inch
  
# jpeg(paste0(plotPrefix, "Figure", "_", plotTitles[i], ".jpeg"), width=8*dpi, height=4*dpi, res=dpi)
  par(mfcol = c(1, 1))
  plotPSS_1 <- pirateplot(
    formula = PSS ~ TestMoment,
    data = data,
    theme = 3,
    pal = "info",
    main = "PSS ~ Testmoment",
    bean.f.o = .6, # Bean fill
    point.o = .3,  # Points
    inf.f.o = .7,  # Inference fill
    inf.b.o = .8,  # Inference border
    avg.line.o = 1,  # Average line
    # bar.f.o = .5, # Bar
    inf.f.col = "white",  # Inf fill col
    inf.b.col = "black",  # Inf border col
    avg.line.col = "black",  # avg line col
    bar.f.col = gray(.8),  # bar filling color
    point.pch = 21,
    point.bg = "white",
    point.col = "black",
    point.cex = .7,
   
    xlab = "",
  )
  
  # Insert visual indicators of significance
x <- 1:2  # capture x coordinates of bars
y <- 42  # create the y coordinate of the line
offset <- 0.2  # set an offset for tick lengths
lines(c(1.05,1.95),c(y, y))  # draw first horizontal line
lines(c(1.05,1.05),c(y, y-offset))  # draw ticks
lines(c(1.95,1.95),c(y, y-offset))
text(x[1]+((x[2]-x[1])/2),y+offset,"***")  # draw asterics

#Bar2
y <- 21
offset <- 0.2
lines(c(2.1,2.9),c(y, y))
lines(c(2.1,2.1),c(y, y-offset))
lines(c(2.9,2.9),c(y, y-offset))
text(x[2]+((x[3]-x[2])/2),y+offset,"***")

#Bar3
y <- 22
offset <- 0.2
lines(c(1.1,2.9),c(y, y))
lines(c(1.1,1.1),c(y, y-offset))
lines(c(2.9,2.9),c(y, y-offset))
text(x[1]+((x[3]-x[1])/2),y+offset,"***")

################################################
  
  plotPSS_2 <- pirateplot(
    formula = PSS ~ PMSScoreNew*TestMoment,
    data = data,
    theme = 3,
    pal = "info",
    main = "PSS ~ PMSScoreNew: Testmoment",
    bean.f.o = .6, # Bean fill
    point.o = .3,  # Points
    inf.f.o = .7,  # Inference fill
    inf.b.o = .8,  # Inference border
    avg.line.o = 1,  # Average line
    # bar.f.o = .5, # Bar
    inf.f.col = "red",  # Inf fill col
    inf.b.col = "black",  # Inf border col
    avg.line.col = "black",  # avg line col
    bar.f.col = grey(.8),  # bar filling color
    point.pch = 21,
    point.bg = "grey",
    point.col = "black",
    point.cex = .7,
   
    xlab = "",
  )

```

with PMSScore=0 and testmoment = 2 there seem to be some outliers
This means that people with low PMS scores had more outliers on testmoment 2. 
we might need to remove some of these outliers



```{r, echo=FALSE}

ZeroLength<- length(which(data$PMSScoreNew=='noPMS'))
OneLength <-length(which(data$PMSScoreNew=='PMS'))


age_mean <- round(mean(data$Age), 1)

summary(data$PSS)

# count for each participants how many trials are left for each PMSScoreNew by PSS
trial_count <- as.data.frame(table(data$Subject, data$PMSScoreNew, data$PSS))
names(trial_count) <- c("subject_id", "PMSScore", "PSS", "count")

trial_mean <- trial_count %>%
  group_by(PMSScore, PSS) %>%
  summarize(
    mean = mean(count),
    sd = sd(count), 
    min = min(count),
    max = max(count)
  )

```
##we have more participants with low PMS score

`r plot(data$PMSScoreNew)`


There are `r ZeroLength` people with low PMS score and `r OneLength` people with high PMS score

The mean age is `r age_mean`

```{r, echo= FALSE}

outliers <- boxplot(data$PSS, plot=FALSE)$out

Q <- quantile(data$PSS, probs= c(.25, .75), na.rm=TRUE)

iqr <-IQR(data$PSS, na.rm=TRUE)
up <- Q[2]+1.5*iqr

low<- Q[1]-1.5*iqr


#data$PSS_without_out<-
PSS_without_out<- data[-which(data$PSS %in% outliers),]


```
## Outliers
`r boxplot(data$PSS, main="some PSS Outliers")$out`
`r outliers`

## Q
`r Q`

## IQR UP
`r up`

## IQR LOW
`r low`


The End! 


############ Hyptothesen en onderzoeksvragen:

Exclude everyone who uses the pill
- Only take participants with contraception == "Natural"

PMS vs noPMS op het gebied van:
- PSS
- BSRI

++ Interactie testmoment
PMS vs noPMS voor:
- DASS_alle subschalen
-- Hoeft niet x testmoment


------- Voor de stimuli -----------
Eerste stap:
- Bekijken Valence en arousal tussen de groepen en testmomenten, over alle stimuli als een geheel

Next:
- Per stimulus gemiddelden plotten
-- Om erachter te komen of er coherent geantwoord wordt
-- Om te zien of ze niet te veel van elkaar afwijken
--- waarchuwing; ze gaan veel van elkaar afwijken. 

Stimuli onderverdelen in negatiev, positief en neutral, om daar een subdivisie analyse op te maken

---------------- Overig:
Normgroepen opzoeken voor PSS bij (jonge) vrouwen



